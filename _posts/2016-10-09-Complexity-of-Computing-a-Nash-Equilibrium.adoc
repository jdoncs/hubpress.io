= Complexity of Computing a Nash Equilibrium
:published_at: 2016-10-09
:hp-tags: Game Theory, Computational Complexity

https://people.csail.mit.edu/costis/simplified.pdf[Paper Here]



== What is this paper about?

In game theory, we study the behaviour of rational agents in various model problems. The classic example is the https://en.wikipedia.org/wiki/Prisoner%27s_dilemma[Prisoner's Dilemma]. Problems in game theory are modeled using the notation of a game. In a game, each actor (player) has a set of possible actions that they could take. Games are played in a single round, where each player picks an action, and an outcome occurs as a function of the joint actions of all the players. Usually in game theory, we're interested in figuring out what rational agents would do when confronted with this problem. We can imagine a rational agent to be a computer program that selects actions to minimize its punishment or maximize its reward.

We might wonder whether there's always an equilibrium where every player just does one of their actions. It is easy to show that there is not. For example, consider the game Matching Pennies. In this game, each player holds a penny concealed in thier hand. The players can elect to arrange the coin so that either heads or tails will be on top when their palm is opened. The outcome of the game is that if both players reveal the same side of the coin, player 1 gets both pennies. If the players reveal different sides of their coins, player 2 gets them both instead. It should be obvious that deciding to always reveal the same side is a recipe for disaster. An opponent who knows that you're doing this could get a penny from you every time! Despite this, there is still a single right way to play this game. Observe that if a player selects heads or tails uniformly at random (i.e. each of them 50% of the time), then it does not matter what their opponent does: the coins will match as often as not. This type of equilibrium is called a mixed strategy https://en.wikipedia.org/wiki/Nash_equilibrium#Definitions[Nash Equilibrium] (because each player picks a mixture of the actions as their strategy for playing the game).

Perhaps the biggest result in game theory is Nash's theorem. Nash's theorem shows something rather exciting: every game has at least one mixed strategy Nash equilibrium. This means that the behaviours of rational agents should always be predictable to some degree (there might be more than one equilibrium, so it might not always be clear which one different rational agents will play). Nash's proof shows the existence of such an equilibrium, but it also gives a method for finding it: if the players each adjust their strategies in turn to exploit whatever their opponents are doing now, then play will eventually converge to an equilibrium. However, the proof does not describe how long players might need to do this before reaching the equilibrium. Nor does it tell us whether there's a faster way to find the equilibrium that avoids this iterative refinement. The main result of this paper by Papadimitriou et al. is to answer these questions. They accomplish this by showing that finding a Nash Equilibrium is at least as hard as some other problems for which no known fast algorithms exist. Surprisingly, these other algorithms are not NP-Complete, as we often expect, but in a more exotic complexity class called PPAD.

=== Complexity

The problem of finding a mixed strategy Nash Equilibrium is known to be easy when the game consists of two players and is zero-sum. When there are more than 2 players, or the game is not zero-sum, we might wonder how hard it is exactly. 

Usually computational complexity is expressed (at least, outside of complexity theory conferences and journals) in terms of the two familiar complexity classes seen in an undergraduate course on algorithms: https://en.wikipedia.org/wiki/P_(complexity)[P] and https://en.wikipedia.org/wiki/NP_(complexity)[NP]. P is the set of problems for which there exists a known algorithm that solves problem instances is a number of operations that is a polynomial function of the input size. For example, if the problem is sorting a list of numbers, the existence of a simple, slow, algorithm like Bubble Sort (which needs a number of operations that is a quadratic function of the length of the list) serves to show that the problem of sorting a list of numbers is in P. 

NP is the set of problems that can be _verified_ in polynomial time. Verification consists of taking a problem instance and proposed solution, and checking whether or not the solution is correct. For example, sorting a list of numbers is in NP because if I give you a list of numbers that are unsorted, and then a list that I claim contains the same numbers in sorted order, you can verify my claim by simply making sure the two lists contain the same numbers (quadratic time if done naively), and then making sure that the second list is indeed sorted (linear time). It should be obvious that every problem in P is also in NP: we can always run the algorithm in a polynomial number of steps, and see if we get a solution. However, there exist problems that can be verified in polynomial time, but for which no polynomial time algorithm is known. 

An example is something like https://en.wikipedia.org/wiki/Knapsack_problem[integer knapsack]. The integer knapsack problem consists of being given a bag with a certain total capacity (in liters, say), and a multiset of items of different sizes that are worth different amounts. The question that is asked is whether there exists a way to pack the knapsack such that the total value of the items in it is at least $$k$$. It should be apparent that brute-forcing this is not an option, since the number of subsets that can fit in the bag is going to be exponential in the size of the multiset. Likewise, it should be obvious that if I present you with a packed bag, it's easy to count up the value of the items in the bag, and see whether that's more than $$k$$ or not. There's no known deterministic polynomial time algorithm for finding such a packed bag though, so the problem is in NP, but not in P.

Finding a Nash Equilibrium is clearly in NP: if I give you a proposed equilibrium, you can check whether any player would want to deviate by computing the gradient of their utility function with respect to the probabilities of playing any given strategy. If the gradient is zero for every player, we're at an equilibrium. Similarly, we can't be sure whether this problem is in P or not, because we don't have an efficient algorithm for solving it (yet). Usually complexity theorists use this starting point to show that a problem is at least as hard as one of the hardest unsolved problems in NP, called the NP-Complete problems. However, most (all?) NP-Complete problems rely on the fact that a solution might not exist,and that proving that there's no solution is very difficult without enumerating all possible solutions. 

My understanding is that this is because the fundamental NP-Complete problem is https://en.wikipedia.org/wiki/Boolean_satisfiability_problem[SAT], and other problems are reduced to SAT. The authors of this paper give an argument that I found a bit handwavy for this. Basically, if we had an efficient way to translate SAT instances (i.e. answering the question of whether or not a particular boolean formula can ever output true), to instances of NASH (Finding an equilibrium where no player can gain more than some fixed amount $$\epsilon$$ by playing differently), then we could solve SAT instances by _guessing_ a solution to NASH, and checking whether that solution was a solution to SAT or not. The authors say it is easy to show that this could yield a non-deterministic algorithm for solving SAT efficiently, but I'm not sure I see how. Obviously solutions will always exist to NASH, since there must be at least one equilibrium with $$\epsilon=0$$ by Nash's theorem. But would _guessing_ really allow one to find solutions quickly? The authors don't say that an efficient non-deterministic algorithm is needed for NASH, but it seems to me like it must be. 

Anyhow, the point is, complexity theorists have some (inscrutable) reason for thinking that guess and check on NASH would allow efficient non-deterministic solutions to SAT. Therefore they suppose that reductions from SAT to NASH are unlikely to exist. I find this uncompelling, but I'm not a complexity theorist, and I certainly can't find such a reduction myself to contradict them! If Nash were NP-Complete then by definition, an instance of SAT could be turned into NASH efficiently _somehow_. Therefore, we suppose NASH is probably not NP-Complete.

So NASH is easier than NP-Complete problems (or at least, incomparable with them), but it's in NP, and we don't have any algorithm that would put it in P after about 50 years of trying. A reasonable guess is that NP > NASH > P. As it turns out, there are some other problems that fit this description. Surprisingly, the authors claim (via one of Papadimitriou's other papers) that the set of possible problems that are similar is quite small. We're just looking for problems that have a non-constructive proof that there is always a solution. The non-constructive step turns out to determine similar sets of problems. PPAD is the set of problems where the non-constructive step in the proof is of the form "If a digraph has one node with different numbers of in- and out-edges, then there must be another such node.". 

The canonical problem for this class (i.e. the SAT of PPAD) is described as "END-OF-THE-LINE": Given a graph, and a particular vertex with different numbers of in- and out-edges, output some other vertex of the graph that also has different numbers of such edges. Of course, if the graph is provided in a standard format this is a boring (and very simple) problem to solve! Just count the number of edges of each type for every vertex. There are at most $$|V|^2$$ directed edges, so work is polynomial.

However, END-OF-THE-LINE is proposed to take a rather arcane input format. A graph is represented by two boolean circuits of depth polynomial in the size of the graph. Each circuit takes $$k$$ binary inputs and produces binary $$k$$ outputs. The graph has $$2^k$$ vertices, and each is constrained to have at most one in-edge and at most one out-edge. The boolean circuits effectively accept a binary number corresponding to a vertex. One of the circuits outputs the target of the out-edge (if any), and the other outputs the target of the in-edge (if any). Oddly, this appears to constrain the graph to having a unique predecessor for each node, even though it could be the successor of many nodes. Weird. The pathological instance of this problem would be to give a source node as the provided vertex with different numbers of in- and out-edges. We know there must be at least one sink in the graph, but since the structure of the graph is encoded only implicitly, we might need to walk through every vertex to find it (e.g. if the "graph" is really a single enormous linked list). Since we cannot examine the boolean circuits (I guess?), this makes END-OF-THE-LINE hard in the worst case, taking apparently exponential time, even though there's certain to be a solution, and solutions are easy to verify. My complaint about this part of the paper is that END-OF-THE-LINE's encoding seems like a pretty contrived way to represent a graph. I guess it's saying: if you can only observe the local topology of a graph (like if you were trapped in a maze), you might have to check every path to find the exit. But the work is still linear in the size of the graph. It's only exponential in $k$, and $k$ just seems like a contrived notion. Mostly the design seems to be a theoretical contrivance, insofar as some other hard problems with input sizes of $k$ can be reduced to END-OF-THE-LINE with $$2^k$$ vertices. I kind of wish they'd said so when presenting END-OF-THE-LINE, but I suppose this is the cost of reading outside one's core area.


== Brouwer's Theorem and PPAD-Hardness


Okay. So we have that NASH is an important problem, and that probably $$NP > NASH > P$$. We also have this weird complexity class PPAD, based around the equally weird problem END-OF-THE-LINE. As one would expect, the meat of the paper is the authors showing that NASH can be converted into END-OF-THE-LINE, and that END-OF-THE-LINE can be converted into NASH. These reductions would establish that NASH is exactly as hard as END-OF-THE-LINE, and frankly, END-OF-THE-LINE seems ridiculously hard (assuming we can't examine the boolean circuits, or otherwise infer the graph's structure except by walking around on it). To accomplish this, they rely on Brouwer's fixed point theorem, which is what's used in the core non-constructive step of Nash's theorem.

https://en.wikipedia.org/wiki/Brouwer_fixed-point_theorem[Brouwer's fixed point theorem] says that if you map any "reasonable" subset of a Euclidian space to a "reasonable" subspace of itself, there's at least one point that doesn't move (i.e. the "fixed" point). Here, "reasonable" means that it's a contiguous proper sub-region of the space. So the unit ball is good (for any number of dimensions), but something like two disjoint balls isn't. If you think about this for a minute, it seems true, though if I think about it for more than a minute I always come up with some weird mapping that seems like it doesn't have a fixed point. Later I always figure out what the fixed point is though. 

Nash's theorem relies on this notion of fixed points. The dimensions of the space are given by the set of probabilities that each player uses to decide which strategy to play. This ends up being some sort of scaling of the unit ball for a high dimensional space, since the probabilities for each player need to sum to 1 (so we should get a ball with radius $$n$$, for $$n$$ players. Suppose that players adjust their strategies to improve utility, given the strategies of their opponents. Then each of these points has a successor point, the strategy profile that the players would move to if they started here. The mapping from points to successors is "reasonable", so by Brouwer's theorem, there's a fixed point, a point where the players don't want to move, which must be a mixed strategy Nash equilibrium.

The authors propose the computational search problem BROUWER, which takes the unit hyper-cube with $$m$$ dimensions, and a polynomial-time computable mapping $$F$$ from points in the cube $$x$$ to other points in the cube $$F(x)$$, and produces a fixed point of the mapping. They do this $$\epsilon$$-approximate fixed point thing again, apparently because algorithms don't do irrational numbers (makes sense: we'd be "computing" forever just to write down an irrational fixed point). They also require that $$F$$ obeys a Lipschitz condition: i.e. if two points $$x$$ and $$y$$ are a distance $$d$$ apart, then $$F(x)$$ and $$F(y)$$ are no more than $$K\times d$$ apart for some constant $$K$$.

To show that BROUWER maps to END-OF-THE-LINE, the authors propose the following technique:

1. Put down a lattice of points over the hypercube, with spacing that "depends" on $$K$$, $$\epsilon$$ and $$m$$. Exactly how this dependency works is not explained. Strangely, this step appears to be exponential in $$\epsilon$$ and $$m$$. The authors don't comment on this at all. I believe that the spacing needs to be such that the distance between diagonally adjacent points in the lattice is no more than $$2*\epsilon$$, but this might not quite be correct.
2. For every point in the lattice $$x$$, compute $$F(x)$$, which is an efficient operation. 
3. Divide the unit ball of dimension $$m$$ into three contiguous regions, and color them red, blue and yellow.
4. Compute the direction of the vector $$F(x) - x$$, and map that vector onto the unit ball. Color lattice point $$x$$ based on the corresponding color from step 3. If $F(x) = x$ for any lattice point then we don't even need to do the rest of this, so don't worry about that case (I think).


Notice that the points along each edge of the hypercube will naturally omit one color: if you're as far left as you can go, then there's no way to map a point to the left of where it is now, for instance. There's a result from combinatorics called https://en.wikipedia.org/wiki/Sperner%27s_lemma[Sperner's Lemma] that says if you make this kind of triangular tessellation of a space, and  color the vertices of the tessellation in this way, one of the triangles has a vertex of every one of the three colors. The Lipschitz condition means that if three points are close enough together (again, I wish they'd be more explicit about the lattice spacing), and yet mapped in three such radically different directions, they're near a fixed point of $$F$$. This kind of makes sense. The Lipzschitz condition ensures that under $F$, the image of the three points all need to be "close" to each other, within some constant multiple of the distance of the three points in the original arrangement. One supposes that if the lattice is arranged such that the points are within $$epsilon/K$$ from each other in the original space (which we can do by making the lattice spacing sufficiently small), then the Lipschitz condition ensures that the three points all have to be with $$\epsilon$$ of each other in the resulting space. So probably the lattice isn't spaced with a distance of $$2\epsilon$$, but with a distance of $$2\epsilon/K$$.  

So now it's easy to convert the problem of finding an $$\epsilon$$-approximate fixed point (BROUWER) to END-OF-THE-LINE. Make a boolean circuit that encodes the direction of $$F(x) - x$$ for any mapping. This should be possible because we assumed $$F$$ was easy to compute. Enumerate  triangles that tesselate the space. There are countably many since the lattice spacing is finite. Build a boolean circuit that maps from each of the triangles to one of its neighbours according to the following rule: If one corner of the triangle is red, and the next corner clockwise from around the parameter is yellow, then create an out edge from the triangle with this number to its neighbour across the edge (here, clockwise just means with respect to some self-consistant view of the points). Notice that this ensures there is at most one out-edge for each triangle, and at most one in-edge for each triangle. You can draw the triangles out to prove this, or just look at this picture from the paper for a while:


image::https://github.com/jdoncs/jdoncs.github.io/raw/master/images/Fig7Exerpt.png[Excerpt from Figure 7 of the paper, to illustrate the triangle colouring.]

Notice that if a triangle has two yellow vertices, or two red vertices, then it has both an in-edge and an out-edge. If it has two blue-vertices, it has no edges at all. However, there exists triangles on the perimeter of the space that _could_ have an in-edge, but only from a region outside the space. Any such triangle is a source. We know any problem will have at least one of these, because Sperner's lemma ensures there's a sink in the graph, and the PPAD observation itself ensures that if there's a sink then somewhere there must be a source. 

So we now imagine we had a fast algorithm for END-OF-THE-LINE, meaning one that was polynomial in $$k$$. We can define boolean circuits to compute these successor relationships with respect to different points in the space. The only other input PPAD requires is a vertex of this graph that has different numbers of in-edges and out-edges. This would have to be a point with 2 yellow and 1 red vertices (or 1 yellow and 2 red), but located on the the perimeter of the space. The authors use a clever trick to ensure that the perimeter of the space has a side that will start with every vertex along the side colored yellow, and at some point transition to every vertex being colored red. The transition point is sure to be a source, and can be found efficiently by doing, e.g. a binary search along the side, though the authors do not explain this part in detail. Anyway, the point is: we can define the triangles and the boolean circuits in polynomial time, and we can find a source vertex in time that is polynomial in the logarithm of the inverse of the lattice spacing. However, the number of triangles is proportionate to the inverse of the lattice spacing raised to the power of the number of dimensions. So this instance of END-OF-THE-LINE has something like $$O(2^{\frac{1}{\epsilon}}$$ nodes. Since we assumed there was an END-OF-THE-LINE algorithm that needed a polynomial number of steps in terms of $$k$$, and translating BROUWER to END-OF-THE-LINE needed only a polynomial number of steps in $$\frac{1}{\epsilon}$$, we can solve BROUWER in a number of steps polynomial in $$\frac{1}{\epsilon}$$. From this, we can conclude that BROUWER is no harder than END-OF-THE-LINE. IF we can solve END-OF-THE-LINE in a number of steps polynomial in $$k$$, we can also solve BROUWER efficiently with respect to $$\epsilon$$.

Of course, we were originally interested in NASH, but it's easy to see how to turn an instance of NASH into an instance of BROUWER (explained earlier in this post), so it should be apparent that NASH is no harder than BROUWER. This means NASH is PPAD-Hard. Any efficient algorithm for problems in the class PPAD-Complete (like END-OF-THE-LINE) can be converted into an efficient algorithm for NASH. 

== NASH is PPAD-Complete

So NASH is no harder than PPAD, but is it any easier?

To show this, the authors first reduce solving an instance of END-OF-THE-LINE to solving an instance of BROUWER. After reading this part of the paper, I understand the gist of this reduction, but the details are described by the authors as "hard", and are left out. The idea is to that we'll be looking for a fixed point in 3-space. The space is partitioned into tiny "cubelets". The centers of the cublets define a lattice, and the lattice nodes are to be colored with one of _four_ colors (0, 1, 2, 3). Initially all nodes are colored "0". The mesh is fine enough so that each of the nodes in the END-OF-THE-LINE graph can be assigned to one cublet on each of the top-left and bottom right corners of the cube. If there is an edge from $$u$$ to $$v$$ in the END-OF-THE-LINE graph, then the coloring of the cublets on the interior of the cube can be defined so that the directions of $$F(x)-x$$ will yield a edge rule much like with the triangular tessellation from earlier, and there is a path formed by these colorings from the top-left point corresponding to $$u$$ to the bottom-left point corresponding to $$v$$. Likewise, the colors can be used to define a path from the bottom-left $$u$$ to the top-left $$v$$. After this encoding is complete, define a function $$F$$ such that $$F(x) - x$$ produces a vector whose's angle can be colored with one of the four colours used for the cublets. The authors claim (without proof here) that such a function can be defined so that it is easy to compute, and that it can be easily interpolated between the centers of the cubelets. If we had an efficient algorithm for BROUWER, we could then run that algorithm on $$F$$ to find a fixed point, and such a fixed point would be a solution to END-OF-THE-LINE when its coordinates were mapped onto the nearest cublet. I can kind of see how this works, but don't want to think too hard about it. The upshot is, BROUWER is PPAD-Complete, since it's no harder than END-OF-THE-LINE, and END-OF-THE-LINE is no harder than BROUWER is.

The final step then is to show that if we had an efficient algorithm for NASH, we could efficiently solve BROUWER. 

=== Games as Boolean Circuits

The first time I read over the paper, I skimmed this part, which seemed almost like a footnote tacked onto the end. However, on closer reading I found this to be the most exciting part! 

Here's the basic idea:

1. Suppose we have an instance of BROUWER with some function $$F$$. Recall that $$F$$ must be easy to compute, with a polynomial-depth boolean circuit.
2. Define a game such that each gate in the boolean circuit representation of $$F$$ is converted to the actions of some subset of players of the game. (Wat?)
3. Define some more players of the game the respectively decide the inputs and outputs for the boolean circuit. Link their payoffs, so that these players are only paid if they adopt identical strategies.
4. Show that, in the Nash equilibrium of this game, the input and output players must have identical values, and the computation players must faithfully implement $F$. This is only possible if the input is a fixed-point of $$F$$. 


So the neat part of this proof was the process of defining a game that does arithmetic. The outline is that $$F$$ can be broken down into just a few kinds of boolean functions, notable addition, multiplication, and comparison. You can make a game that computes each of these, and then compose these games together into a larger game.

The paper gives a nice example with for computing $$Z=X*Y$$. We define 4 players $$w,x,y,z$$. Each player can play one of two actions, "STOP" and "GO". Their strategy is then defined as a probability ($$W,X,Y,Z$$) of playing GO. We do not pay $$x$$ or $$y$$ anything in this subgame, so they'll use whatever values they like. Usually these values will be defined by some other game. $$w$$ gets paid $$X \times Y$$ if it plays STOP, and $$Z$$ if it plays GO. $$z$$ gets paid $$X\times Y$$ for playing GO, and $$W$$ for playing STOP.  The unique equilibrium for this game is for $$w$$ to play $$GO$$ with probability $$X\times Y$$, and $$z$$ to play GO with probability $$X\times Y$$. Thus, if $$x$$ and $$y$$'s probabilities of playing GO were fixed, then the probability that $$z$$ plays GO is always $$X\times Y$$. We can then define the interior connections of the boolean circuit by connecting, e.g. $$z$$ as the $$x$$ or $$y$$ player in some other circuit game.

The input of $F$ is a three-dimension value in a finite cube. Simply map the range $$(0,1)$$ onto each axis of the cube, and define three players, one for each dimension. Like the others, they play either STOP or GO. Define three more points as players at the output of the $$F$$ circuit in the same way. The payoffs for the six input players are set so that they are only in equilibrium when the three input players and the three output players represent the same point with their probabilities.

The big catch with this is that if our circuit for $$F$$ had a polynomial number of gates $$n$$, then we have $$n+6$$ players each with a binary action, and thus a game with $$2^{n+6}$$ payoffs that need to be encoded. It's not obvious that a game with exponential size like this can be compactly encoded. If it can't, then the reduction from BROUWER to NASH is not polynomial time, and so even if we had a fast algorithm for NASH, we would still do exponential work to solve BROUWER (since we'd do exponential work just to convert an instance of BROUWER into an instance of NASH).

To fix this, the authors show that the game can be reduced to one played between 3 players. Basically, each boolean circuit will have some input players, some middle player, and some output player. As long as each of these groups is controlled by a different player, the circuit will end up in the right equilibrium. The authors show that you can color the players such that only three colors are needed, and therefore the game can be played by three people, each selecting between a linear number of actions that (Somehow? This point is not well explained) encode the exponential number of actions their gates might produce. The upshot is, any instance of BROUWER can be reduced to NASH for a 3-player game, so NASH with 3 players must be PPAD-Complete. Of course, it's easy to make this maping with more than 3 players (just add some dummy players that don't interact with the main 3). This means that NASH must be PPAD-Complete when there are 3 or more players.


== Other Tidbits

The authors mention Bibelus as an author who showed that any game played among more than 3 players could be reduced to a game among exactly 3. Their results show this in a different way (I think: Convert the n-player game to its END-OF-THE-LINE instance, then convert that END-OF-THE-LINE instance into NASH for 3 players; a fixed point in this 3-player game can be converted back into a fixed point in the original game. Weird!). I think I should read the paper by Bibelus in the future.

The authors also reference a paper by Chen and Deng, that shows a much more surprising result: the circuits created by converting BROUWER to NASH never contain Multiply gates, and so can actually be colored using just _2_ colors. This implies that the problem of finding an equilibrium in any n-player game can be efficiently converted into the problem of finding an equilibrium in a 2-player game, which seems ludicrous on the surface, but makes sense the more I think about it.


== So What?

NASH is PPAD-Complete. PPAD looks hard (in fact, it seems there isn't even a good stochastic approximation algorithm right now?).

Practically, this means it's hard to predict how rational agents might play a game. Real-world games are pretty complex beasts (say, the global economy), so if our algorithms for solving them scale exponentially, then we probably can't do much of anything.

Much more important is a point the authors raise: if it's not tractable to find fixed points, then why would we suppose that agents would (could?) play strategies that lie at a fixed point? That is, if in general Nash equilibria cannot be found without exponential computational efforts, then does finding a Nash Equilibrium actually tell us much of anything? Maybe the whole solution concept is kind of useless.

The paper is also 8 years old. I haven't heard anything about efficient algorithms (approximate or otherwise) for NASH, but I do wonder what sort of work people have done since towards this. Probably a reverse citation search on the paper would make it pretty apparent.

I wonder too about the implications of quantum computers for PPAD. I know NP and BQP overlap, but are not subsets of one another. Where exactly does PPAD sit relative to BQP? It certainly _seems_ like the sort of thing that would be easy with something like Grover's algorithm, because we'd have a polynomial depth circuit to act as an oracle. However, we're not looking something that matches a signature, but something that's unchanged when it goes through the function. Have people worked on this topic? Maybe I should ask Chris Grenade what he thinks about this.




==== Why did I pick this paper?

This is an older paper I've been meaning to read for a couple of years. One of the authors is Christos Papadimitriou. Papadimitriou does a lot of exciting work. The two things that come to my mind are co-inventing https://en.wikipedia.org/wiki/Price_of_anarchy[Price of Anarchy], a metric for the cost of having self-interested (and rational) individuals do as they please, rather than imposing a solution on all of them from some central authority. It's very useful in mechanism design, because it allows us to quantify the improvement in social utility from using a given mechanism, vs. allowing the agents to settle into whatever the usual Nash Equilibrium is for the game of interest. Papadimitriou has also done some work in genetic algorithms, which purports to show that mutation is entirely pointless in evolutionary search. I haven't read the paper on this yet. Perhaps it should be my next review.




